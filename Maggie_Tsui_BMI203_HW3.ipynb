{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import *\n",
    "class NeuralNetwork:\n",
    "    \"\"\"Class that creates a NN and includes methods to train and test\"\"\"\n",
    "    def __init__(self, setup=[[68,25,\"sigmoid\",0],[25,1,\"sigmoid\",0]],lr=.05,seed=1,error_rate=0,bias=0.5,iters=500,lamb=.00001,simple=0):\n",
    "        #Note - these paramaters are examples, not the required init function parameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._error_rate = error_rate\n",
    "        self._bias = bias\n",
    "        self._iters = iters\n",
    "        self._lamb = lamb\n",
    "        self._simple = simple\n",
    "        \n",
    "\n",
    "        # network is represented as a list of layers,\n",
    "        # where layers are a list of nodes, where nodes\n",
    "        # are a list of weights.\n",
    "        # weights = [ [[w1,w2...], [w1,w2...]] <- layer1\n",
    "        #             [[w1,w2...], [w1,w2...]] <- layer2\n",
    "        #           ]\n",
    "        weights = []\n",
    "        outputs = []\n",
    "        change = []\n",
    "        \n",
    "        # initialize the given number of layers with weights\n",
    "        for layer in setup:\n",
    "            weights.append(self.make_weights(n_inputs=layer[0],n_nodes=layer[1]))\n",
    "            outputs.append([0] * layer[1])\n",
    "            change.append([0] * layer[1])\n",
    "        \n",
    "        self._weights = weights\n",
    "        self._outputs = outputs\n",
    "        self._change = change\n",
    "        \n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, lr):\n",
    "        self._lr = lr\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self._bias\n",
    "\n",
    "    @bias.setter\n",
    "    def bias(self, bias):\n",
    "        self._bias = bias \n",
    "    \n",
    "    @property\n",
    "    def seed(self):\n",
    "        return self._seed\n",
    "\n",
    "    @seed.setter\n",
    "    def seed(self, seed):\n",
    "        self._seed = seed \n",
    "    \n",
    "    @property\n",
    "    def outputs(self):\n",
    "        return self._outputs\n",
    "\n",
    "    @outputs.setter\n",
    "    def outputs(self, outputs):\n",
    "        self._outputs = outputs \n",
    "        \n",
    "    @property\n",
    "    def change(self):\n",
    "        return self._change\n",
    "\n",
    "    @change.setter\n",
    "    def change(self, change):\n",
    "        self._change = change \n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self._weights\n",
    "\n",
    "    @weights.setter\n",
    "    def weights(self, weights):\n",
    "        self._weights = weights \n",
    "        \n",
    "        \n",
    "    def make_weights(self,n_inputs, n_nodes):\n",
    "        \"\"\"\n",
    "        Generates random weights for the network initialization\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        n_inputs\n",
    "            Number of input nodes to this layer\n",
    "        n_nodes\n",
    "            Number of nodes to generate weights for\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "        Layer with random weights initialized for each node\n",
    "        \"\"\"\n",
    "        #seed(self.seed)\n",
    "        layer = []\n",
    "        \n",
    "        # Get n_inputs random float between -1 and 1 for each node\n",
    "        for i in range(n_nodes):\n",
    "            node_weights = [uniform(-1, 1) for j in range(n_inputs)]\n",
    "            node_weights.append(self.bias) # add bias at end\n",
    "            layer.append(node_weights)\n",
    "        \n",
    "        return layer\n",
    "\n",
    "    def feedforward(self, data):\n",
    "        \"\"\"\n",
    "        Takes in data and passes it through the NN\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        data\n",
    "            One datapoint\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "        The output(s) of the final layer in the network\n",
    "        \"\"\"\n",
    "        inputs = data\n",
    "        \n",
    "        # pass data through all layers\n",
    "        for layer in range(len(self.weights)):\n",
    "            next_inputs = []\n",
    "            for node in range(len(self.weights[layer])):\n",
    "                sum = 0\n",
    "                for i in range(len(inputs)): # multiply inputs by weights and add to sum\n",
    "                    sum += inputs[i]*self.weights[layer][node][i]\n",
    "                    \n",
    "                sum += self.weights[layer][node][-1] # add bias\n",
    "                output = sigmoid(sum) # Apply activation function\n",
    "                self.outputs[layer][node] = output\n",
    "                next_inputs.append(output)\n",
    "            inputs = next_inputs\n",
    "        # inputs should now be the final layer output\n",
    "        return inputs\n",
    "    \n",
    "    def backprop(self, true_values, data):\n",
    "        \"\"\"\n",
    "        Calculates the loss and gradient for each output node.\n",
    "        Propagates the gradient through the network and records\n",
    "        the error for each node.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        true_values\n",
    "            true classification of example\n",
    "        data\n",
    "            training example\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        None, change matrix is filled in for weight updating\n",
    "        \"\"\"\n",
    "        # start at last layer\n",
    "        for layer in reversed(range(len(self.outputs))): \n",
    "            if layer == len(self.outputs) - 1: # for last layer, calculate loss using true values\n",
    "                for node in range(len(self.outputs[layer])):\n",
    "                    loss = (true_values[node] - self.outputs[layer][node])\n",
    "                    # fill in change matrix\n",
    "                    self.change[layer][node] = loss*sigmoid_derivative(self.outputs[layer][node])\n",
    "            else: # for all other layers\n",
    "                for node in range(len(self.outputs[layer])):\n",
    "                    loss = 0\n",
    "                    # sum weighted losses from previous layer\n",
    "                    for prev_layer_node in range(len(self.weights[layer + 1])):\n",
    "                        loss += self.weights[layer+1][prev_layer_node][node]*self.change[layer+1][prev_layer_node]\n",
    "                    # fill in change matrix\n",
    "                    self.change[layer][node] = loss*sigmoid_derivative(self.outputs[layer][node])\n",
    "         \n",
    "        \n",
    "        # Update weights\n",
    "        for layer in reversed(range(len(self.outputs))): \n",
    "            input = data[0] # the input to the first layer is the training example\n",
    "            if layer != 0: # the input to rest of layers is output of prev layer\n",
    "                input = [self.outputs[layer-1][node] for node in range(len(self.outputs[layer - 1]))]\n",
    "            for node in range(len(self.outputs[layer])):\n",
    "                for i in range(len(input)):\n",
    "                    self.weights[layer][node][i] += self.lr*self.change[layer][node]*input[i]\n",
    "                # update bias\n",
    "                self.weights[layer][node][-1] += self.lr*self.change[layer][node]\n",
    "\n",
    "\n",
    "    def fit(self, training):\n",
    "        \"\"\"\n",
    "        Trains the neural network and computes training loss.\n",
    "        After each epoch, computes the training and validation loss.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        true_values\n",
    "            a list of true value(s) associated with the current\n",
    "            training example\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        Dataframe of \n",
    "        \"\"\"\n",
    "        train_loss = 0\n",
    "        for row in training:\n",
    "            output = self.feedforward(row[0])\n",
    "            expected = row[-1] # Expected value should be last element of training row\n",
    "            # Sum loss of all output nodes\n",
    "            train_loss += sum([(expected[i]-output[i])**2 for i in range(len(expected))])\n",
    "            self.backprop(true_values=expected, data=row)\n",
    "        return train_loss/len(training)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        return self.feedforward(data)\n",
    "\n",
    "def activation(input, weights):\n",
    "    pass\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_seqs(pos_file, neg_file):\n",
    "    \"\"\"\n",
    "    Reads in positive and negative sequences from paths\n",
    "    into two lists, pos and neg. For negative sequences,\n",
    "    skips the lines starting with \">\"\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    pos_file\n",
    "        path to positive examples\n",
    "    neg_file\n",
    "        path to negative examples\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    two lists, where each element is a sequence from the file\n",
    "    \"\"\"\n",
    "    with open(pos_file) as f:\n",
    "        pos = f.read().splitlines()\n",
    "    \n",
    "    neg = []\n",
    "    seq = ''\n",
    "    for line in open(neg_file):\n",
    "        if line.startswith(\">\"):\n",
    "            if seq != '':\n",
    "                neg.append(seq)\n",
    "                seq = ''\n",
    "        else:\n",
    "            seq += line.strip() \n",
    "    neg.append(seq)\n",
    "    return pos,neg\n",
    "\n",
    "def encode_seq(sequence):\n",
    "    \"\"\"\n",
    "    Performs one-hot encoding of a nucleotide sequence,\n",
    "    where each nucleotide is represented by a binary vector\n",
    "    of length 4, ie: [1, 0, 0, 0], where a 1 corresponds to\n",
    "    which nucleotide it is: [A, C, G, T]\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    sequence\n",
    "        the sequence string to encode\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    A one-hot encoded sequence represented as a list of lists,\n",
    "    each with length 4\n",
    "    \"\"\"\n",
    "    encoded = []\n",
    "    for nuc in sequence:\n",
    "        if nuc == 'A':\n",
    "            encoded+=[1,0,0,0]\n",
    "        elif nuc == 'C':\n",
    "            encoded+=[0,1,0,0]\n",
    "        elif nuc == 'G':\n",
    "            encoded+=[0,0,1,0]\n",
    "        elif nuc == 'T':\n",
    "            encoded+=[0,0,0,1]\n",
    "    return encoded\n",
    "\n",
    "def train_val_split():\n",
    "    \"\"\"\n",
    "    Randomly split the positive and negative examples \n",
    "    into training and validation sets at an 80/20 ratio.\n",
    "    Since there are less pos examples, we are limited by\n",
    "    the \n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    sequence\n",
    "        the sequence string to encode\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    A one-hot encoded sequence represented as a list of lists,\n",
    "    each with length 4\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_make_weights():\n",
    "    nn = NeuralNetwork(setup=[[8,3,\"sigmoid\",0],[3,8,\"sigmoid\",0]])\n",
    "    assert len(nn.weights) == 2\n",
    "    assert len(nn.weights[0]) == 3\n",
    "    assert len(nn.weights[1]) == 8\n",
    "    assert len(nn.weights[0][0]) == 9\n",
    "    assert len(nn.weights[1][0]) == 4\n",
    "\n",
    "def test_feedforward():\n",
    "    nn = NeuralNetwork([[2,1, \"sigmoid\",0], [1,2, \"sigmoid\",0]])\n",
    "    # a 2x1x2 network\n",
    "    out = nn.feedforward([1,1])\n",
    "    assert len(out) == 2\n",
    "    assert nn.outputs[1] == out\n",
    "    \n",
    "def test_encoder():\n",
    "    assert True\n",
    "\n",
    "def test_encoder_relu():\n",
    "    assert True\n",
    "\n",
    "def test_one_d_ouput():\n",
    "    assert True\n",
    "\n",
    "def test_read_train_seqs():\n",
    "    pos,neg = read_train_seqs(pos_file = \"./data/rap1-lieb-positives.txt\", neg_file = \"./data/yeast-upstream-1k-negative.fa\")\n",
    "    print(len(pos),len(neg))\n",
    "    assert len(neg) == 3164\n",
    "    assert len(pos) == 137\n",
    "    for i in range(len(pos)):\n",
    "        assert len(pos[i]) == 17\n",
    "\n",
    "def test_encode_seq():\n",
    "    seq = 'ACTG'\n",
    "    encoded = encode_seq(seq)\n",
    "    assert len(encoded) == 16\n",
    "    assert encoded == [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encode_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([[8,3, \"sigmoid\",0], [3,8, \"sigmoid\",0]])\n",
    "# a 2x1x2 network\n",
    "nn.feedforward([1,0,0,0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feedforward()\n",
    "test_make_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.backprop(true_values = [1,0,0,0,0,0,0,1], data = [[1,0,0,0,0,0,0,1], [1,0,0,0,0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.feedforward([1,0,0,0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(columns = ['Epoch', 'Train', 'Validation']) \n",
    "for epoch in range(n_epochs):\n",
    "    nn.fit(train,test, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 8 bit binary vectors\n",
    "identity = list(np.identity(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]],\n",
       " [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]],\n",
       " [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]],\n",
       " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]],\n",
       " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = [[list(i),list(i)] for i in identity]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([[8,3, \"sigmoid\",0], [3,8, \"sigmoid\",0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(columns = ['Epoch', 'Train']) \n",
    "nn.lr = 0.3\n",
    "for epoch in range(2000):\n",
    "    if epoch == 5000:\n",
    "        nn.lr = 0.05\n",
    "    loss = nn.fit(train)\n",
    "    losses = losses.append({'Epoch' : epoch, 'Train' : loss}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f92f5de79a0>"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "for name in ['Train']:\n",
    "    ax.plot(losses['Epoch'],losses[name], label=name)\n",
    "\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.74092782768302e-06,\n",
       " 0.027704145086872255,\n",
       " 0.8929093848505144,\n",
       " 7.777271801085917e-08,\n",
       " 1.3662143966493838e-05,\n",
       " 0.015151432420715599,\n",
       " 3.4111013022065914e-05,\n",
       " 0.0021706500592901883]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict([0,0,1,0,0,0,0,0]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NN for TF binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAATCTTCACCACCCAA',\n",
       " 'AGTAAATAACAGATAAT',\n",
       " 'CATTGTAAAGGAAAACC',\n",
       " 'AAAATAATAGGTGTAAA']"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives, full_negs = read_train_seqs(pos_file=\"./data/rap1-lieb-positives.txt\", neg_file=\"./data/yeast-upstream-1k-negative.fa\")\n",
    "\n",
    "# just take the last 17 bp of all of the negatives\n",
    "negatives = [seq[len(seq)-17:len(seq)] for seq in full_negs]\n",
    "negatives[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = [[encode_seq(seq),[1]] for seq in positives]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_enc = [[encode_seq(seq),[0]] for seq in negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put some examples in a validation set. \n",
    "# 80/20 split for positives: Since there are only 137 positives, let's save 27 positives.\n",
    "# Let's just save 64 negs for validation.\n",
    "\n",
    "pos_test_len = 27\n",
    "pos_test_idx = sample(range(len(pos_enc)), pos_test_len)\n",
    "neg_test_len = 64\n",
    "neg_test_idx = sample(range(len(neg_enc)), neg_test_len)\n",
    "\n",
    "pos_test = [pos_enc[idx] for idx in pos_test_idx]\n",
    "neg_test = [neg_enc[idx] for idx in neg_test_idx]\n",
    "\n",
    "pos_train = [pos_enc[idx] for idx in range(len(pos_enc)) if idx not in pos_test_idx]\n",
    "neg_train = [neg_enc[idx] for idx in range(len(neg_enc)) if idx not in neg_test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "\n",
    "# make two queues\n",
    "pos_queue = queue.Queue(maxsize=0) \n",
    "  \n",
    "for i in range(len(pos_train)):\n",
    "    pos_queue.put(pos_train[i])\n",
    "\n",
    "neg_queue = queue.Queue(maxsize=0) \n",
    "  \n",
    "for i in range(len(neg_train)):\n",
    "    neg_queue.put(neg_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_batch_size = 50\n",
    "neg_batch_size = 155\n",
    "n_epochs = 1\n",
    "losses = pd.DataFrame(columns = ['Epoch', 'Train', 'Validation']) \n",
    "\n",
    "nn = NeuralNetwork([[68,25, \"sigmoid\",0], [25,1, \"sigmoid\",0]])\n",
    "nn.lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-326-6a341951dd71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Average over the batch losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-ccbe0faebdfc>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, training)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;31m# Sum loss of all output nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-ccbe0faebdfc>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, true_values, data)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the input to the first layer is the training example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# the input to rest of layers is output of prev layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-ccbe0faebdfc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the input to the first layer is the training example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# the input to rest of layers is output of prev layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    batch_losses = []\n",
    "    # run through all neg examples while upsampling pos examples\n",
    "    while neg_queue.empty() == False:\n",
    "        # get a new batch of positive and neg examples\n",
    "        neg_batch = [neg_queue.get() for i in range(neg_batch_size)]\n",
    "        pos_batch = []\n",
    "        for i in range(pos_batch_size):\n",
    "            if pos_queue.empty(): # replenesh pos samples when necessary\n",
    "                for i in range(len(pos_train)):\n",
    "                    pos_queue.put(pos_train[i])\n",
    "            pos_batch.append(pos_queue.get())\n",
    "        \n",
    "        train = neg_batch + pos_batch\n",
    "        shuffle(train)\n",
    "        \n",
    "        batch_losses.append(nn.fit(train))\n",
    "        \n",
    "    # Average over the batch losses    \n",
    "    train_loss = sum(batch_losses)/len(batch_losses)\n",
    "    \n",
    "    for val in validation:\n",
    "        output = self.predict(val[0])\n",
    "        expected = val[-1]\n",
    "        val_loss += sum([(expected[i]-output[i])**2 for i in range(len(expected))])\n",
    "    losses = losses.append({'Epoch' : epoch, 'Train' : train_loss,\n",
    "                            'Validation': val_loss/len(validation)}, ignore_index=True)\n",
    "    \n",
    "    # shuffle negative examples and add to queue\n",
    "    shuffle(neg_train)\n",
    "    for i in range(len(neg_train)):\n",
    "        neg_queue.put(neg_train[i])\n",
    "        \n",
    "    shuffle(pos_train)\n",
    "    pos_queue.queue.clear()\n",
    "    for i in range(len(pos_train)):\n",
    "        pos_queue.put(pos_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nn.outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0],\n",
       " 0]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 1, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "shuffle(m)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0, 1], 1]\n"
     ]
    }
   ],
   "source": [
    "positives = queue.Queue(maxsize=0) \n",
    "  \n",
    "positives.put([[1,0,0,0,1],1])\n",
    "positives.put([[1,0,0,0,1],0])\n",
    "positives.put([[1,0,0,1,1],1])\n",
    "\n",
    "print(positives.get(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neg_queue' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-244-115615188559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneg_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'neg_queue' is not defined"
     ]
    }
   ],
   "source": [
    "neg_queue.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<queue.Queue at 0x7f92f347d6d0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 1], 0]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACATCCGTGCACCTCCG'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq ='ACATCCGTGCACCTCCG'\n",
    "seq[len(seq)-17:len(seq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 1.])]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.identity(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
