{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import queue\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from random import *\n",
    "\n",
    "from scripts.NN import *\n",
    "from scripts.io import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Autoencoder implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 8 bit binary vectors\n",
    "identity = list(np.identity(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [[list(i),list(i)] for i in identity]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([[8,3, \"sigmoid\"], [3,8, \"sigmoid\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(columns = ['Epoch', 'Train']) \n",
    "nn.lr = 0.3\n",
    "for epoch in range(5000):\n",
    "    loss = nn.fit(train)\n",
    "    losses = losses.append({'Epoch' : epoch, 'Train' : loss}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "for name in ['Train']:\n",
    "    ax.plot(losses['Epoch'],losses[name], label=name)\n",
    "\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass all 8 vectors in, showing input and the output\n",
    "# sorry this is so ugly\n",
    "for i in identity:\n",
    "    print(\"input: \",list(i))\n",
    "    print(\"output: \",nn.predict(list(i)), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Adapt for classification, and develop training regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a) Describe your process of encoding your training DNA sequences into input vectors in detail. Include a description of how you think the representation might affect your networkâ€™s predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For positive training examples, I used all 17 bp each. For the negative examples, I took the last 17 bp of each sequence. Since the neg sequences are the 1000 bp upstream of a yeast gene, I reasoned that the last 17 bp should be closest to the TSS. My hope was that these 17 bp would fall in or around an area that a transcription factor would normally bind. I'm not sure if this logic is correct, but it was a straightforward way to subset the long sequences. I think it is important for both the positive and negative examples to be potential TF binding sites so that the network learns what distinguishes a Rap1 binding site from other binding sites, rather than what distinguishes a transcription factor binding site in general. \n",
    "\n",
    "For the encoding, each nucleotide in a DNA sequence is represented as a binary vector of length 4. Example: [1, 0, 0, 0], where a 1 corresponds to which nucleotide it is: [A, C, G, T]. Thus, to get the entire sequence I concatenate these vectors together, resulting in a vector of length 4 * 17. Example: [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]. This one-hot encoding will place equal weight on each nucleotide, rather than place more importance on some nucleotides than others. There may be some relationship between the frequency of one type of nucleotide and Rap1 binding, but I went with one-hot encoding to be safe. \n",
    "\n",
    "All of the positive examples were classified as 1, and neg examples classified as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAATCTTCACCACCCAA',\n",
       " 'AGTAAATAACAGATAAT',\n",
       " 'CATTGTAAAGGAAAACC',\n",
       " 'AAAATAATAGGTGTAAA']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives, full_negs = read_train_seqs(pos_file=\"./data/rap1-lieb-positives.txt\", neg_file=\"./data/yeast-upstream-1k-negative.fa\")\n",
    "\n",
    "# just take the last 17 bp of all of the negatives\n",
    "negatives = [seq[len(seq)-17:len(seq)] for seq in full_negs]\n",
    "negatives[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = [[encode_seq(seq),[1]] for seq in positives]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_enc = [[encode_seq(seq),[0]] for seq in negatives]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. a) Describe your training regime. How was your training regime designed so as to prevent the negative training data from overwhelming the positive training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first split the data into 80% training, 20% validation. I tried to ensure that the validation set had a good proportion of both positives and negatives. For each epoch, I split the training data into batches, with each batch containing a balanced number of positive and negative examples. Since there are a lot more negative examples than positives, I wanted to make sure the model saw all of the negative examples while still having class-balanced batches. Thus, I re-used some of the positive examples more than once per epoch. To do this, I used two separate queues of training data--one for positive examples and one for negative examples. To create each batch, I dispense a fixed number of positives and negatives from each queue. The epoch ends when the negative queue runs out. The positive queue is replenished with examples when needed.\n",
    "\n",
    "I also wanted to prevent the model from seeing the same ordering of examples in batches every time. To combat this I repopulated and shuffled the queues after each epoch so that the same positive and negative examples wouldn't always be in the same batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put some examples in a validation set. \n",
    "# 80/20 split for positives: Since there are only 137 positives, let's save 27 positives.\n",
    "# Let's just save 64 negs for validation.\n",
    "\n",
    "pos_test_len = 27\n",
    "pos_test_idx = sample(range(len(pos_enc)), pos_test_len)\n",
    "neg_test_len = 64\n",
    "neg_test_idx = sample(range(len(neg_enc)), neg_test_len)\n",
    "\n",
    "pos_test = [pos_enc[idx] for idx in pos_test_idx]\n",
    "neg_test = [neg_enc[idx] for idx in neg_test_idx]\n",
    "\n",
    "pos_train = [pos_enc[idx] for idx in range(len(pos_enc)) if idx not in pos_test_idx]\n",
    "neg_train = [neg_enc[idx] for idx in range(len(neg_enc)) if idx not in neg_test_idx]\n",
    "\n",
    "validation = pos_test + neg_test\n",
    "shuffle(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([[68,25, \"sigmoid\"], [25,1, \"sigmoid\"]])\n",
    "losses = training(pos_batch_size = 50,neg_batch_size = 155, \n",
    "                  n_epochs = 10, nn = nn, neg_train=neg_train, pos_train = pos_train, validation=validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. a) Provide an example of the input and output for one true positive sequence and one true negative sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive sequence\n",
    "print(\"input: \", pos_enc[0][0])\n",
    "print(\"output: \",nn.predict(pos_enc[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg sequence\n",
    "print(\"input: \", neg_enc[0][0])\n",
    "print(\"output: \",nn.predict(neg_enc[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. b) Describe your network architecture, and the results of your training. How did your network perform in terms of minimizing error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a network with 68 input nodes, a hidden layer with 25 nodes, and 1 output node. I used all sigmoid activations. My lr was 0.05, and I trained for 10 epochs. Both the training and validation losses steadily decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "for name in ['Train', 'Validation']:\n",
    "    ax.plot(losses['Epoch'],losses[name], label=name)\n",
    "\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. c) What was your stop criterion for convergence in your learned parameters? How did you decide this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the validation loss goes below, 0.004, the training stops. I chose 0.004 because I ran training for a range of epochs and calculated the AUC on the validation set, and it seems to reach an AUC of 0.99 at that value. If the loss was any lower, the network is not learning much more. Also, I am realizing that I did this wrong and should have probably used a held-out test set that the model has never seen to calculate the AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. a) How can you use k-fold cross validation to determine your modelâ€™s performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation ensures that the network isn't overfitting to a particular training and validation split. It splits the training data into k folds and trains k different networks where the validation set is the k-th fold, and the training set is everything minus that k-th fold. You can then average the AUCs for each fold to determine the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. b) Given the size of your dataset, positive and negative examples, how would you select a value for k?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like popular values of k are 3, 5, and 10. I chose 5 so that the network can see a variety of different splits, but also so that the positive examples aren't spread too thin across the folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. c) Using the selected value of k, determine a relevant metric of performance for each fold. Describe how your model performed under cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "all_preds, all_actuals = cross_validation(k = k, pos_batch_size=50, neg_batch_size = 211, n_epochs = 1, pos_enc = pos_enc, neg_enc = neg_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fpr, tpr, and auc for each model\n",
    "result_table = pd.DataFrame(columns=['k', 'fpr','tpr','auc'])\n",
    "\n",
    "for i in range(k):  \n",
    "    fpr, tpr, _ = roc_curve(all_actuals[i],  all_preds[i])\n",
    "    auc = roc_auc_score(all_actuals[i], all_preds[i])\n",
    "    \n",
    "    result_table = result_table.append({'k': i,\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc}, ignore_index=True)\n",
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC \n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "for i in result_table.index:\n",
    "    plt.plot(result_table.loc[i]['fpr'], \n",
    "             result_table.loc[i]['tpr'], \n",
    "             label=\"fold={}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying some hyperparameter optimization using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a few different architectures, lrs, and change up the positive batch size per iteration\n",
    "architectures = [[[68,25, \"sigmoid\"], [25,10, \"sigmoid\"], [10,1, \"sigmoid\"]],\n",
    "                 [[68,10, \"sigmoid\"], [10,5, \"sigmoid\"], [5,1, \"sigmoid\"]],\n",
    "                 [[68,10, \"sigmoid\"], [10,1, \"sigmoid\"]]\n",
    "                ]\n",
    "lrs = [0.1, 0.05, 0.01]\n",
    "pos_batch_size = [30, 50, 80]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "result_table = pd.DataFrame(columns=['arch', 'lr','pos_batch_size','avg_auc'])\n",
    "\n",
    "for arch in architectures:\n",
    "    for lr in lrs:\n",
    "        for pbs in pos_batch_size:\n",
    "            nn = NeuralNetwork(setup = arch, lr = lr)\n",
    "            all_preds, all_actuals = cross_validation(k = k, pos_batch_size=pbs,neg_batch_size = 211, n_epochs = 5, pos_enc = pos_enc, neg_enc = neg_enc)\n",
    "            aucs = 0\n",
    "            for i in range(k):  \n",
    "                auc = roc_auc_score(all_actuals[i], all_preds[i])\n",
    "                aucs+=auc\n",
    "            avg_auc = aucs/k\n",
    "            result_table = result_table.append({'arch': arch,\n",
    "                                        'lr':lr, \n",
    "                                        'pos_batch_size':pbs, \n",
    "                                        'avg_auc':avg_auc}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>lr</th>\n",
       "      <th>pos_batch_size</th>\n",
       "      <th>avg_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.999849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>80</td>\n",
       "      <td>0.999859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0.999730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>80</td>\n",
       "      <td>0.999919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>50</td>\n",
       "      <td>0.999759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>80</td>\n",
       "      <td>0.999732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.999824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>80</td>\n",
       "      <td>0.999930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0.999942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>80</td>\n",
       "      <td>0.999802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>50</td>\n",
       "      <td>0.999836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>80</td>\n",
       "      <td>0.999977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 1, sigmoid]]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 1, sigmoid]]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.999931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 1, sigmoid]]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>80</td>\n",
       "      <td>0.999909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 1, sigmoid]]</td>\n",
       "      <td>0.05</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 1, sigmoid]]</td>\n",
       "      <td>0.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0.999766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 1, sigmoid]]</td>\n",
       "      <td>0.05</td>\n",
       "      <td>80</td>\n",
       "      <td>0.999906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 1, sigmoid]]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 1, sigmoid]]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>50</td>\n",
       "      <td>0.999906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[[68, 10, sigmoid], [10, 1, sigmoid]]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>80</td>\n",
       "      <td>0.999906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 arch    lr pos_batch_size  \\\n",
       "0   [[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...  0.10             30   \n",
       "1   [[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...  0.10             50   \n",
       "2   [[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...  0.10             80   \n",
       "3   [[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...  0.05             30   \n",
       "4   [[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...  0.05             50   \n",
       "5   [[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...  0.05             80   \n",
       "6   [[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...  0.01             30   \n",
       "7   [[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...  0.01             50   \n",
       "8   [[68, 25, sigmoid], [25, 10, sigmoid], [10, 1,...  0.01             80   \n",
       "9   [[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...  0.10             30   \n",
       "10  [[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...  0.10             50   \n",
       "11  [[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...  0.10             80   \n",
       "12  [[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...  0.05             30   \n",
       "13  [[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...  0.05             50   \n",
       "14  [[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...  0.05             80   \n",
       "15  [[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...  0.01             30   \n",
       "16  [[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...  0.01             50   \n",
       "17  [[68, 10, sigmoid], [10, 5, sigmoid], [5, 1, s...  0.01             80   \n",
       "18              [[68, 10, sigmoid], [10, 1, sigmoid]]  0.10             30   \n",
       "19              [[68, 10, sigmoid], [10, 1, sigmoid]]  0.10             50   \n",
       "20              [[68, 10, sigmoid], [10, 1, sigmoid]]  0.10             80   \n",
       "21              [[68, 10, sigmoid], [10, 1, sigmoid]]  0.05             30   \n",
       "22              [[68, 10, sigmoid], [10, 1, sigmoid]]  0.05             50   \n",
       "23              [[68, 10, sigmoid], [10, 1, sigmoid]]  0.05             80   \n",
       "24              [[68, 10, sigmoid], [10, 1, sigmoid]]  0.01             30   \n",
       "25              [[68, 10, sigmoid], [10, 1, sigmoid]]  0.01             50   \n",
       "26              [[68, 10, sigmoid], [10, 1, sigmoid]]  0.01             80   \n",
       "\n",
       "     avg_auc  \n",
       "0   0.999861  \n",
       "1   0.999849  \n",
       "2   0.999859  \n",
       "3   0.999944  \n",
       "4   0.999730  \n",
       "5   0.999919  \n",
       "6   0.999885  \n",
       "7   0.999759  \n",
       "8   0.999732  \n",
       "9   0.999872  \n",
       "10  0.999824  \n",
       "11  0.999930  \n",
       "12  0.999774  \n",
       "13  0.999942  \n",
       "14  0.999802  \n",
       "15  0.999836  \n",
       "16  0.999836  \n",
       "17  0.999977  \n",
       "18  0.999906  \n",
       "19  0.999931  \n",
       "20  0.999909  \n",
       "21  0.999839  \n",
       "22  0.999766  \n",
       "23  0.999906  \n",
       "24  0.999953  \n",
       "25  0.999906  \n",
       "26  0.999906  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_table.to_csv(\"./hyperparameter_tuning_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluate your network on the final set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in test set\n",
    "with open(\"./data/rap1-lieb-test.txt\") as f:\n",
    "        test = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc = [encode_seq(seq) for seq in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_len = 27\n",
    "pos_test_idx = sample(range(len(pos_enc)), pos_test_len)\n",
    "neg_test_len = 64\n",
    "neg_test_idx = sample(range(len(neg_enc)), neg_test_len)\n",
    "\n",
    "pos_test = [pos_enc[idx] for idx in pos_test_idx]\n",
    "neg_test = [neg_enc[idx] for idx in neg_test_idx]\n",
    "\n",
    "pos_train = [pos_enc[idx] for idx in range(len(pos_enc)) if idx not in pos_test_idx]\n",
    "neg_train = [neg_enc[idx] for idx in range(len(neg_enc)) if idx not in neg_test_idx]\n",
    "\n",
    "validation = pos_test + neg_test\n",
    "shuffle(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to use optimal hyperparameters from part 4\n",
    "nn = NeuralNetwork([[68,25, \"sigmoid\"], [25,1, \"sigmoid\"]])\n",
    "losses = training(pos_batch_size = 50,neg_batch_size = 155, n_epochs = 10, nn = nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model on test set and print outputs to file\n",
    "with open('predictions.txt', 'w') as f:\n",
    "    for t in range(len(test_enc)):\n",
    "        print(test[t], \"\\t\", nn.predict(test_enc[t][0])[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
